{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run mathutil.ipynb\n",
    "np.random.seed(1234)\n",
    "def randomize(): np.random.seed(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 클래스 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, name, dataset):\n",
    "        self.name = name\n",
    "        self.dataset = dataset\n",
    "        self.is_training = False #학습 중 여부를 나타내는 플래그\n",
    "        if not hasattr(self, 'rand_std'):\n",
    "            self.rand_std = 0.030\n",
    "    \n",
    "    def __str__(self): #출력문으로 객체를 출력할 때의 출력 문자열 생성방법 정의\n",
    "        return '{}/{}'.format(self.name, self.dataset)\n",
    "\n",
    "    def exec_all(self, epoch_count = 10, batch_size = 10, learning_rate = 0.001, report = 0, show_cnt = 3):\n",
    "        #전체 과정을 실행시키는 메인 함수\n",
    "        self.train(epoch_count, batch_size, learning_rate, report)\n",
    "        self.test()\n",
    "        if show_cnt > 0: self.visualize(show_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MlpModel 클래스 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpModel(Model): #Model의 파생 클래스로 선언\n",
    "    #객체 초기화 메서드 __init__을 제외한 나머지 메서드들을 클래스 선언 밖에서 따로 함수를 정의해 메서드로 등록하는 방식\n",
    "    def __init__(self, name, dataset, hconfigs):\n",
    "        super(MlpModel, self).__init__(name, dataset) #super 명령을 이용해 기반 클래스인 Model클래스를 찾아 그 객체 초기화 함수를 호출하여 name, dataset 저장\n",
    "        self.init_parameters(hconfigs) #신경망이 이용할 파라미터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다층 퍼셉트론의 은닉 계층 구성은 init_parameters 메서드에 전달되는 hconfig 인숫값에 따라 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터 생성 메서드의 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_init_parameters(self, hconfigs):\n",
    "    self.hconfigs = hconfigs\n",
    "    self.pm_hiddens = []\n",
    "\n",
    "    prev_shape = self.dataset.input_shape #입출력 벡터의 크기 등의 정보는 전역변수가 아닌 dataset 객체의 속성값으로 얻음\n",
    "\n",
    "    for hconfig in hconfigs:\n",
    "        pm_hidden, prev_shape = self.alloc_layer_param(prev_shape, hconfig)\n",
    "        self.pm_hiddens.append(pm_hidden) #생성된 파라미터들을 전역 변수에 저장하는 대신 객체변수로 저장\n",
    "\n",
    "    output_cnt = int(np.prod(self.dataset.output_shape)) #파라미터 생성에 반영되는 벡터 크기를 np.prod 함수를 이용해 변환\n",
    "    #확장 과정에서 입출력 벡터 크기나 은닉 벡터 크기가 자연수가 아닌 다차원 크기를 나타내는 리스트나 튜플의 형채로도 표현되기 때문\n",
    "    self.pm_output, _ = self.alloc_layer_param(prev_shape, output_cnt)\n",
    "\n",
    "def mlp_alloc_layer_param(self, input_cnt, output_cnt):\n",
    "    input_cnt = np.prod(input_cnt)\n",
    "    output_cnt = hconfig\n",
    "\n",
    "    weight, bias = self.alloc_param_pair([input_cnt, output_cnt])\n",
    "    return {'w':weight, 'b':bias}, output_cnt\n",
    "\n",
    "def mlp_alloc_param_pair(self, shape):\n",
    "    weight = np.random.normal(0, self.rand_std, shape)\n",
    "    bias = np.zeros([shape[-1]])\n",
    "    return weight, bias\n",
    "\n",
    "MlpModel.init_parameters = mlp_init_parameters\n",
    "MlpModel.alloc_layer_param = mlp_alloc_layer_param\n",
    "MlpModel.alloc_param_pair = mlp_alloc_param_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init_parameters 메서드는 직접 모든 일을 처리하지 않고 계층 하나의 파라미터를 생성할 때마다 alloc_layer_param 메서드를 부름\n",
    "\n",
    "alloc_layer_param 역시 alloc_param_pair메서드를 불러 가중치와 편향 파라미터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model_train(self, epoch_count = 10, batch_size = 10, learning_rate = 0.001, report= 0):\n",
    "    self.learning_rate = learning_rate\n",
    "\n",
    "    batch_count = int(self.dataset.train_count / batch_size)\n",
    "    time1 = time2 = int(time.time())\n",
    "    if report != 0:\n",
    "        print('Model {} train started:'.format(self.name))\n",
    "\n",
    "    for epoch in epoch_count:\n",
    "        costs = []\n",
    "        accs = []\n",
    "        self.dataset.shuffle_train_data(batch_size * batch_count)\n",
    "\n",
    "        for n in range(batch_count):\n",
    "            trX, trY = self.dataset.get_train_data(batch_size, n)\n",
    "            cost, acc = self.train_step(trX, trY)\n",
    "            costs.append(cost)\n",
    "            accs.append(acc)\n",
    "\n",
    "        if report > 0 and (epoch + 1) % report == 0:\n",
    "            vaX, vaY = self.dataset.get_validate_data(100)\n",
    "            acc = self.eval_accuracy(vaX, vaY)\n",
    "            time3 = int(time.time())\n",
    "            tm1, tm2 = time3-time2, time3-time1\n",
    "            self.dataset.train_prt_result(epoch+1, costs, accs, acc, tm1, tm2)\n",
    "            time2 = time3\n",
    "\n",
    "    tm_total = int(time.time() - time1)\n",
    "    print('Model () train ended in () secs:'.format(self.name, tm_total))\n",
    "\n",
    "MlpModel.train = mlp_model_train\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "690cbce85060ece167cf4f98205a6f94239bf2060ab864022b18747bcaf29842"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
